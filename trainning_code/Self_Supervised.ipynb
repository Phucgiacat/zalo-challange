{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --q ultralytics supervision torch torchvision transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mteTLczaTTZN",
        "outputId": "8ab16989-9b1c-4448-d238-2fd88d257608"
      },
      "id": "mteTLczaTTZN",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.0/1.1 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/212.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m212.4/212.4 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!gdown --q --folder 1n98ur5MdsKuRtXTHkeM4PEwklBkDaJe4\n",
        "!unzip --q /content/Data/observing.zip\n",
        "!unzip --q /content/Data/public_test.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8HfAU0mTa9A",
        "outputId": "523eb6f0-2f80-4d93-a9e2-7ce36b5942be"
      },
      "id": "M8HfAU0mTa9A",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/Data/observing.zip\n",
            "   creating: train/\n",
            "   creating: train/samples/\n",
            "   creating: train/samples/Backpack_0/\n",
            "   creating: train/samples/Backpack_0/object_images/\n",
            "  inflating: train/samples/Backpack_0/object_images/img_1.jpg  \n",
            "  inflating: train/samples/Backpack_0/object_images/img_2.jpg  \n",
            "  inflating: train/samples/Backpack_0/object_images/img_3.jpg  \n",
            "  inflating: train/samples/Backpack_0/drone_video.mp4  \n",
            "   creating: train/samples/Backpack_1/\n",
            "   creating: train/samples/Backpack_1/object_images/\n",
            "  inflating: train/samples/Backpack_1/object_images/img_1.jpg  \n",
            "  inflating: train/samples/Backpack_1/object_images/img_2.jpg  \n",
            "  inflating: train/samples/Backpack_1/object_images/img_3.jpg  \n",
            "  inflating: train/samples/Backpack_1/drone_video.mp4  \n",
            "   creating: train/samples/Jacket_0/\n",
            "   creating: train/samples/Jacket_0/object_images/\n",
            "  inflating: train/samples/Jacket_0/object_images/img_1.jpg  \n",
            "  inflating: train/samples/Jacket_0/object_images/img_2.jpg  \n",
            "  inflating: train/samples/Jacket_0/object_images/img_3.jpg  \n",
            "  inflating: train/samples/Jacket_0/drone_video.mp4  \n",
            "   creating: train/samples/Jacket_1/\n",
            "   creating: train/samples/Jacket_1/object_images/\n",
            "  inflating: train/samples/Jacket_1/object_images/img_1.jpg  \n",
            "  inflating: train/samples/Jacket_1/object_images/img_2.jpg  \n",
            "  inflating: train/samples/Jacket_1/object_images/img_3.jpg  \n",
            "  inflating: train/samples/Jacket_1/drone_video.mp4  \n",
            "   creating: train/samples/Laptop_0/\n",
            "   creating: train/samples/Laptop_0/object_images/\n",
            "  inflating: train/samples/Laptop_0/object_images/img_1.jpg  \n",
            "  inflating: train/samples/Laptop_0/object_images/img_2.jpg  \n",
            "  inflating: train/samples/Laptop_0/object_images/img_3.jpg  \n",
            "  inflating: train/samples/Laptop_0/drone_video.mp4  \n",
            "   creating: train/samples/Laptop_1/\n",
            "   creating: train/samples/Laptop_1/object_images/\n",
            "  inflating: train/samples/Laptop_1/object_images/img_1.jpg  \n",
            "  inflating: train/samples/Laptop_1/object_images/img_2.jpg  \n",
            "  inflating: train/samples/Laptop_1/object_images/img_3.jpg  \n",
            "  inflating: train/samples/Laptop_1/drone_video.mp4  \n",
            "   creating: train/samples/MobilePhone_0/\n",
            "   creating: train/samples/MobilePhone_0/object_images/\n",
            "  inflating: train/samples/MobilePhone_0/object_images/img_1.jpg  \n",
            "  inflating: train/samples/MobilePhone_0/object_images/img_2.jpg  \n",
            "  inflating: train/samples/MobilePhone_0/object_images/img_3.jpg  \n",
            "  inflating: train/samples/MobilePhone_0/drone_video.mp4  \n",
            "   creating: train/samples/MobilePhone_1/\n",
            "   creating: train/samples/MobilePhone_1/object_images/\n",
            "  inflating: train/samples/MobilePhone_1/object_images/img_1.jpg  \n",
            "  inflating: train/samples/MobilePhone_1/object_images/img_2.jpg  \n",
            "  inflating: train/samples/MobilePhone_1/object_images/img_3.jpg  \n",
            "  inflating: train/samples/MobilePhone_1/drone_video.mp4  \n",
            "   creating: train/samples/Person1_0/\n",
            "   creating: train/samples/Person1_0/object_images/\n",
            "  inflating: train/samples/Person1_0/object_images/img_1.jpg  \n",
            "  inflating: train/samples/Person1_0/object_images/img_2.jpg  \n",
            "  inflating: train/samples/Person1_0/object_images/img_3.jpg  \n",
            "  inflating: train/samples/Person1_0/drone_video.mp4  \n",
            "   creating: train/samples/Person1_1/\n",
            "   creating: train/samples/Person1_1/object_images/\n",
            "  inflating: train/samples/Person1_1/object_images/img_1.jpg  \n",
            "  inflating: train/samples/Person1_1/object_images/img_2.jpg  \n",
            "  inflating: train/samples/Person1_1/object_images/img_3.jpg  \n",
            "  inflating: train/samples/Person1_1/drone_video.mp4  \n",
            "   creating: train/samples/WaterBottle_0/\n",
            "   creating: train/samples/WaterBottle_0/object_images/\n",
            "  inflating: train/samples/WaterBottle_0/object_images/img_1.jpg  \n",
            "  inflating: train/samples/WaterBottle_0/object_images/img_2.jpg  \n",
            "  inflating: train/samples/WaterBottle_0/object_images/img_3.jpg  \n",
            "  inflating: train/samples/WaterBottle_0/drone_video.mp4  \n",
            "   creating: train/samples/WaterBottle_1/\n",
            "   creating: train/samples/WaterBottle_1/object_images/\n",
            "  inflating: train/samples/WaterBottle_1/object_images/img_1.jpg  \n",
            "  inflating: train/samples/WaterBottle_1/object_images/img_2.jpg  \n",
            "  inflating: train/samples/WaterBottle_1/object_images/img_3.jpg  \n",
            "  inflating: train/samples/WaterBottle_1/drone_video.mp4  \n",
            "   creating: train/samples/Lifering_0/\n",
            "   creating: train/samples/Lifering_0/object_images/\n",
            "  inflating: train/samples/Lifering_0/object_images/img_1.jpg  \n",
            "  inflating: train/samples/Lifering_0/object_images/img_2.jpg  \n",
            "  inflating: train/samples/Lifering_0/object_images/img_3.jpg  \n",
            "  inflating: train/samples/Lifering_0/drone_video.mp4  \n",
            "   creating: train/samples/Lifering_1/\n",
            "   creating: train/samples/Lifering_1/object_images/\n",
            "  inflating: train/samples/Lifering_1/object_images/img_1.jpg  \n",
            "  inflating: train/samples/Lifering_1/object_images/img_2.jpg  \n",
            "  inflating: train/samples/Lifering_1/object_images/img_3.jpg  \n",
            "  inflating: train/samples/Lifering_1/drone_video.mp4  \n",
            "   creating: train/annotations/\n",
            "  inflating: train/annotations/annotations.json  \n",
            "Archive:  /content/Data/public_test.zip\n",
            "   creating: public_test/\n",
            "   creating: public_test/samples/\n",
            "   creating: public_test/samples/BlackBox_0/\n",
            "   creating: public_test/samples/BlackBox_0/object_images/\n",
            "  inflating: public_test/samples/BlackBox_0/object_images/img_1.jpg  \n",
            "  inflating: public_test/samples/BlackBox_0/object_images/img_2.jpg  \n",
            "  inflating: public_test/samples/BlackBox_0/object_images/img_3.jpg  \n",
            "  inflating: public_test/samples/BlackBox_0/drone_video.mp4  \n",
            "   creating: public_test/samples/BlackBox_1/\n",
            "   creating: public_test/samples/BlackBox_1/object_images/\n",
            "  inflating: public_test/samples/BlackBox_1/object_images/img_1.jpg  \n",
            "  inflating: public_test/samples/BlackBox_1/object_images/img_2.jpg  \n",
            "  inflating: public_test/samples/BlackBox_1/object_images/img_3.jpg  \n",
            "  inflating: public_test/samples/BlackBox_1/drone_video.mp4  \n",
            "   creating: public_test/samples/CardboardBox_0/\n",
            "   creating: public_test/samples/CardboardBox_0/object_images/\n",
            "  inflating: public_test/samples/CardboardBox_0/object_images/img_1.jpg  \n",
            "  inflating: public_test/samples/CardboardBox_0/object_images/img_2.jpg  \n",
            "  inflating: public_test/samples/CardboardBox_0/object_images/img_3.jpg  \n",
            "  inflating: public_test/samples/CardboardBox_0/drone_video.mp4  \n",
            "   creating: public_test/samples/CardboardBox_1/\n",
            "   creating: public_test/samples/CardboardBox_1/object_images/\n",
            "  inflating: public_test/samples/CardboardBox_1/object_images/img_1.jpg  \n",
            "  inflating: public_test/samples/CardboardBox_1/object_images/img_2.jpg  \n",
            "  inflating: public_test/samples/CardboardBox_1/object_images/img_3.jpg  \n",
            "  inflating: public_test/samples/CardboardBox_1/drone_video.mp4  \n",
            "   creating: public_test/samples/LifeJacket_0/\n",
            "   creating: public_test/samples/LifeJacket_0/object_images/\n",
            "  inflating: public_test/samples/LifeJacket_0/object_images/img_1.jpg  \n",
            "  inflating: public_test/samples/LifeJacket_0/object_images/img_2.jpg  \n",
            "  inflating: public_test/samples/LifeJacket_0/object_images/img_3.jpg  \n",
            "  inflating: public_test/samples/LifeJacket_0/drone_video.mp4  \n",
            "   creating: public_test/samples/LifeJacket_1/\n",
            "   creating: public_test/samples/LifeJacket_1/object_images/\n",
            "  inflating: public_test/samples/LifeJacket_1/object_images/img_1.jpg  \n",
            "  inflating: public_test/samples/LifeJacket_1/object_images/img_2.jpg  \n",
            "  inflating: public_test/samples/LifeJacket_1/object_images/img_3.jpg  \n",
            "  inflating: public_test/samples/LifeJacket_1/drone_video.mp4  \n",
            "   creating: public_test/.ipynb_checkpoints/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46dea78a",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46dea78a",
        "outputId": "d81c4a19-91d9-4a6e-c303-fed7387368e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ‚úÖ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "‚úÖ Configuration loaded\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "COMPLETE OPTIMIZED PIPELINE: YOLO World + Masked Frame Training (Mixed Curriculum)\n",
        "T·ªëi ∆∞u ƒë·ªÉ gi·∫£m s·ªë tham s·ªë xu·ªëng d∆∞·ªõi 50M\n",
        "\"\"\"\n",
        "\n",
        "# ============================================\n",
        "# IMPORTS\n",
        "# ============================================\n",
        "\n",
        "import os\n",
        "import json\n",
        "import cv2\n",
        "import yaml\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from scipy.interpolate import interp1d\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from ultralytics import YOLOWorld\n",
        "\n",
        "# ============================================\n",
        "# CONFIGURATION (ƒê√É T·ªêI ∆ØU)\n",
        "# ============================================\n",
        "\n",
        "class Config:\n",
        "    \"\"\"C·∫•u h√¨nh cho pipeline training\"\"\"\n",
        "    # Dataset paths\n",
        "    DATASET_ROOT = \"train\"\n",
        "    ANNOTATIONS_PATH = os.path.join(DATASET_ROOT, \"annotations/annotations.json\")\n",
        "    SAMPLES_DIR = os.path.join(DATASET_ROOT, \"samples\")\n",
        "    WORK_DIR = \"enhanced_mixed_dataset\"\n",
        "\n",
        "    # Training settings\n",
        "    TRAIN_RATIO = 0.8\n",
        "    IMG_EXT = \"jpg\"\n",
        "    FRAME_STEP = 1\n",
        "    NUM_WORKERS = 8\n",
        "\n",
        "    # ========================================\n",
        "    # üî• THAY ƒê·ªîI QUAN TR·ªåNG: Model nh·ªè h∆°n\n",
        "    # ========================================\n",
        "    MODEL_WEIGHTS = \"yolov8s-worldv2.pt\"  # ‚úÖ ~11M params base (thay v√¨ yolov8m 28M)\n",
        "    # N·∫øu mu·ªën c·ª±c nh·ªè: \"yolov8n-worldv2.pt\"  # ‚úÖ ~3M params base\n",
        "\n",
        "    CLASS_NAMES = [\"target\"]\n",
        "\n",
        "    # Parameter limit\n",
        "    PARAM_LIMIT = 50_000_000  # 50M params limit\n",
        "\n",
        "    # Masked training settings\n",
        "    ENABLE_MASKING = True\n",
        "    NUM_AUGMENTATIONS_PER_PHASE = 5\n",
        "\n",
        "    # Curriculum Learning settings\n",
        "    CURRICULUM = {\n",
        "        'phase1': {'epochs': (0, 15), 'mask_ratio': 0.1, 'strategy': 'random'},\n",
        "        'phase2': {'epochs': (15, 35), 'mask_ratio': 0.3, 'strategy': 'span'},\n",
        "        'phase3': {'epochs': (35, 50), 'mask_ratio': 0.5, 'strategy': 'keyframe'}\n",
        "    }\n",
        "\n",
        "config = Config()\n",
        "print(\"‚úÖ Configuration loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfb510b5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfb510b5",
        "outputId": "26d4033d-4529-4cb1-cb29-a1114d5f9338"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Model optimizer class loaded\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# MODEL OPTIMIZER CLASS\n",
        "# ============================================\n",
        "\n",
        "class ModelOptimizer:\n",
        "    \"\"\"T·ªëi ∆∞u model YOLO World\"\"\"\n",
        "\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "\n",
        "    def count_parameters(self):\n",
        "        \"\"\"ƒê·∫øm s·ªë tham s·ªë\"\"\"\n",
        "        model = self.model.model if hasattr(self.model, 'model') else self.model\n",
        "\n",
        "        total = sum(p.numel() for p in model.parameters())\n",
        "        trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "        return {\n",
        "            'total': total,\n",
        "            'trainable': trainable,\n",
        "            'non_trainable': total - trainable\n",
        "        }\n",
        "\n",
        "    def print_model_summary(self, stage=\"\"):\n",
        "        \"\"\"In th·ªëng k√™ model\"\"\"\n",
        "        params = self.count_parameters()\n",
        "\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"üìä MODEL STATISTICS {stage}\")\n",
        "        print(f\"{'='*60}\")\n",
        "        print(f\"   Total params: {params['total']:,}\")\n",
        "        print(f\"   Trainable: {params['trainable']:,}\")\n",
        "        print(f\"   Non-trainable: {params['non_trainable']:,}\")\n",
        "        print(f\"   Memory (FP32): {params['total'] * 4 / 1024**2:.2f} MB\")\n",
        "        print(f\"   Memory (FP16): {params['total'] * 2 / 1024**2:.2f} MB\")\n",
        "\n",
        "        # Check limit\n",
        "        if params['total'] > config.PARAM_LIMIT:\n",
        "            print(f\"\\n   ‚ùå EXCEEDED LIMIT: {params['total'] - config.PARAM_LIMIT:,} params over\")\n",
        "            print(f\"   Limit: {config.PARAM_LIMIT:,}\")\n",
        "        else:\n",
        "            print(f\"\\n   ‚úÖ WITHIN LIMIT: {config.PARAM_LIMIT - params['total']:,} params remaining\")\n",
        "\n",
        "        print(f\"{'='*60}\\n\")\n",
        "\n",
        "        return params\n",
        "\n",
        "    def apply_magnitude_pruning(self, prune_ratio=0.3):\n",
        "        \"\"\"\n",
        "        Magnitude-based pruning: X√≥a weights c√≥ gi√° tr·ªã tuy·ªát ƒë·ªëi nh·ªè\n",
        "        \"\"\"\n",
        "        print(f\"\\nüîß Applying {prune_ratio*100:.0f}% Magnitude Pruning...\")\n",
        "\n",
        "        model = self.model.model if hasattr(self.model, 'model') else self.model\n",
        "\n",
        "        # Thu th·∫≠p t·∫•t c·∫£ weights\n",
        "        all_weights = []\n",
        "        for name, module in model.named_modules():\n",
        "            if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
        "                if hasattr(module, 'weight') and module.weight is not None:\n",
        "                    all_weights.append(module.weight.data.abs().flatten())\n",
        "\n",
        "        if not all_weights:\n",
        "            print(\"‚ö†Ô∏è No weights found for pruning!\")\n",
        "            return\n",
        "\n",
        "        # T√≠nh threshold\n",
        "        all_weights_tensor = torch.cat(all_weights)\n",
        "        threshold = torch.quantile(all_weights_tensor, prune_ratio)\n",
        "\n",
        "        print(f\"   Threshold: {threshold:.6f}\")\n",
        "\n",
        "        # √Åp d·ª•ng pruning\n",
        "        pruned_count = 0\n",
        "        total_count = 0\n",
        "\n",
        "        for name, module in model.named_modules():\n",
        "            if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
        "                if hasattr(module, 'weight') and module.weight is not None:\n",
        "                    weight = module.weight.data\n",
        "                    mask = weight.abs() > threshold\n",
        "                    weight.mul_(mask)\n",
        "\n",
        "                    pruned_count += (~mask).sum().item()\n",
        "                    total_count += weight.numel()\n",
        "\n",
        "        print(f\"   ‚úÖ Pruned {pruned_count:,} / {total_count:,} weights \"\n",
        "              f\"({pruned_count/total_count*100:.2f}%)\")\n",
        "\n",
        "        return self.model\n",
        "\n",
        "print(\"‚úÖ Model optimizer class loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2555a321",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2555a321",
        "outputId": "6e44ebef-5691-47b6-c831-2f8c959ba8da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Masking strategies loaded\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# 1. MASKING STRATEGIES\n",
        "# ============================================\n",
        "\n",
        "class FrameMaskingStrategy:\n",
        "    \"\"\"C√°c chi·∫øn l∆∞·ª£c che frames ƒë·ªÉ t·∫°o self-supervised learning\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def random_mask(total_frames, mask_ratio=0.3):\n",
        "        \"\"\"Che ng·∫´u nhi√™n mask_ratio% frames\"\"\"\n",
        "        if total_frames < 3:\n",
        "            return []\n",
        "        num_mask = max(1, int(total_frames * mask_ratio))\n",
        "        num_mask = min(num_mask, total_frames - 2)  # Keep at least 2 visible\n",
        "        if num_mask <= 0:\n",
        "            return []\n",
        "        try:\n",
        "            masked_indices = random.sample(range(1, total_frames-1), num_mask)\n",
        "            return sorted(masked_indices)\n",
        "        except ValueError:\n",
        "            # N·∫øu kh√¥ng ƒë·ªß frames ƒë·ªÉ sample\n",
        "            return []\n",
        "\n",
        "    @staticmethod\n",
        "    def span_mask(total_frames, mask_ratio=0.3):\n",
        "        \"\"\"Che c√°c ƒëo·∫°n li√™n ti·∫øp\"\"\"\n",
        "        if total_frames < 5:\n",
        "            return FrameMaskingStrategy.random_mask(total_frames, mask_ratio)\n",
        "\n",
        "        span_length = max(2, int(total_frames * 0.1))\n",
        "        num_spans = max(1, int(total_frames * mask_ratio / span_length))\n",
        "\n",
        "        masked_indices = []\n",
        "        for _ in range(num_spans):\n",
        "            if total_frames - span_length - 1 <= 1:\n",
        "                start = 1\n",
        "            else:\n",
        "                start = random.randint(1, max(2, total_frames - span_length - 1))\n",
        "            masked_indices.extend(range(start, min(start + span_length, total_frames-1)))\n",
        "\n",
        "        return sorted(list(set(masked_indices)))\n",
        "\n",
        "    @staticmethod\n",
        "    def keyframe_mask(frame_boxes, mask_ratio=0.3):\n",
        "        \"\"\"∆Øu ti√™n mask frames c√≥ chuy·ªÉn ƒë·ªông l·ªõn\"\"\"\n",
        "        if len(frame_boxes) < 3:\n",
        "            return []\n",
        "\n",
        "        # Calculate motion scores\n",
        "        motion_scores = []\n",
        "        frame_indices = sorted(frame_boxes.keys())\n",
        "\n",
        "        for i in range(1, len(frame_indices) - 1):\n",
        "            prev_frame = frame_indices[i-1]\n",
        "            curr_frame = frame_indices[i]\n",
        "\n",
        "            if not frame_boxes.get(prev_frame) or not frame_boxes.get(curr_frame):\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                prev_box = frame_boxes[prev_frame][0]\n",
        "                curr_box = frame_boxes[curr_frame][0]\n",
        "\n",
        "                # Calculate center displacement\n",
        "                prev_cx = (prev_box['x1'] + prev_box['x2']) / 2\n",
        "                prev_cy = (prev_box['y1'] + prev_box['y2']) / 2\n",
        "                curr_cx = (curr_box['x1'] + curr_box['x2']) / 2\n",
        "                curr_cy = (curr_box['y1'] + curr_box['y2']) / 2\n",
        "\n",
        "                motion = np.sqrt((curr_cx - prev_cx)**2 + (curr_cy - prev_cy)**2)\n",
        "                motion_scores.append((curr_frame, motion))\n",
        "            except (IndexError, KeyError):\n",
        "                continue\n",
        "\n",
        "        if not motion_scores:\n",
        "            return FrameMaskingStrategy.random_mask(len(frame_indices), mask_ratio)\n",
        "\n",
        "        # Sort by motion and mask top frames\n",
        "        motion_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "        num_mask = max(1, int(len(motion_scores) * mask_ratio))\n",
        "        masked_frames = [frame for frame, _ in motion_scores[:num_mask]]\n",
        "\n",
        "        return sorted(masked_frames)\n",
        "\n",
        "    @staticmethod\n",
        "    def block_mask(total_frames, mask_ratio=0.3):\n",
        "        \"\"\"Che theo blocks\"\"\"\n",
        "        if total_frames < 5:\n",
        "            return FrameMaskingStrategy.random_mask(total_frames, mask_ratio)\n",
        "\n",
        "        block_size = max(3, int(total_frames * 0.05))\n",
        "        num_blocks = total_frames // block_size\n",
        "        if num_blocks < 3:\n",
        "            return FrameMaskingStrategy.random_mask(total_frames, mask_ratio)\n",
        "\n",
        "        num_mask_blocks = max(1, int(num_blocks * mask_ratio))\n",
        "\n",
        "        try:\n",
        "            masked_blocks = random.sample(range(1, num_blocks-1),\n",
        "                                          min(num_mask_blocks, num_blocks-2))\n",
        "        except ValueError:\n",
        "            return []\n",
        "\n",
        "        masked_indices = []\n",
        "        for block_idx in masked_blocks:\n",
        "            start = block_idx * block_size\n",
        "            end = min(start + block_size, total_frames - 1)\n",
        "            masked_indices.extend(range(start, end))\n",
        "\n",
        "        return sorted(masked_indices)\n",
        "\n",
        "print(\"‚úÖ Masking strategies loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d49266b8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d49266b8",
        "outputId": "d360670b-17e2-4fb4-cac5-4eda1a58906e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Temporal interpolation function loaded\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# 2. TEMPORAL INTERPOLATION\n",
        "# ============================================\n",
        "\n",
        "def interpolate_boxes(frame_boxes, masked_frames, method='cubic'):\n",
        "    \"\"\"\n",
        "    N·ªôi suy bounding boxes cho frames b·ªã mask\n",
        "\n",
        "    Args:\n",
        "        frame_boxes: Dict mapping frame_idx -> list of bbox dicts\n",
        "        masked_frames: List of frame indices to interpolate\n",
        "        method: Interpolation method ('linear', 'cubic', etc.)\n",
        "\n",
        "    Returns:\n",
        "        ground_truth: Dict of interpolated boxes for masked frames\n",
        "        visible_frames: List of visible frame indices\n",
        "    \"\"\"\n",
        "    all_frame_indices = sorted(frame_boxes.keys())\n",
        "    visible_frames = [f for f in all_frame_indices if f not in masked_frames]\n",
        "\n",
        "    if len(visible_frames) < 2:\n",
        "        return {}, all_frame_indices  # Cannot interpolate\n",
        "\n",
        "    ground_truth = {}\n",
        "\n",
        "    # Extract coordinates from visible frames (assume single object)\n",
        "    visible_data = [(f, frame_boxes[f][0]) for f in visible_frames\n",
        "                    if frame_boxes[f] and len(frame_boxes[f]) > 0]\n",
        "\n",
        "    if len(visible_data) < 2:\n",
        "        return {}, visible_frames\n",
        "\n",
        "    try:\n",
        "        frames = [d[0] for d in visible_data]\n",
        "        x1_vals = [d[1]['x1'] for d in visible_data]\n",
        "        y1_vals = [d[1]['y1'] for d in visible_data]\n",
        "        x2_vals = [d[1]['x2'] for d in visible_data]\n",
        "        y2_vals = [d[1]['y2'] for d in visible_data]\n",
        "\n",
        "        # Create interpolation functions\n",
        "        kind = 'linear' if len(frames) == 2 else method\n",
        "        f_x1 = interp1d(frames, x1_vals, kind=kind, bounds_error=False,\n",
        "                        fill_value=(x1_vals[0], x1_vals[-1]))\n",
        "        f_y1 = interp1d(frames, y1_vals, kind=kind, bounds_error=False,\n",
        "                        fill_value=(y1_vals[0], y1_vals[-1]))\n",
        "        f_x2 = interp1d(frames, x2_vals, kind=kind, bounds_error=False,\n",
        "                        fill_value=(x2_vals[0], x2_vals[-1]))\n",
        "        f_y2 = interp1d(frames, y2_vals, kind=kind, bounds_error=False,\n",
        "                        fill_value=(y2_vals[0], y2_vals[-1]))\n",
        "\n",
        "        # Generate ground truth for masked frames\n",
        "        for frame_idx in masked_frames:\n",
        "            if frame_idx in frame_boxes:  # Only interpolate existing frames\n",
        "                ground_truth[frame_idx] = [{\n",
        "                    'frame': frame_idx,\n",
        "                    'x1': int(np.clip(f_x1(frame_idx), 0, 10000)),\n",
        "                    'y1': int(np.clip(f_y1(frame_idx), 0, 10000)),\n",
        "                    'x2': int(np.clip(f_x2(frame_idx), 0, 10000)),\n",
        "                    'y2': int(np.clip(f_y2(frame_idx), 0, 10000))\n",
        "                }]\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Interpolation failed: {e}\")\n",
        "        return {}, visible_frames\n",
        "\n",
        "    return ground_truth, visible_frames\n",
        "\n",
        "print(\"‚úÖ Temporal interpolation function loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e910df80",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e910df80",
        "outputId": "bb451c9e-2c15-41c1-8437-bfa68eb0ecf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Duplicate removal function loaded\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# 3. REMOVE DUPLICATE BOXES\n",
        "# ============================================\n",
        "\n",
        "def remove_duplicate_boxes(boxes, iou_threshold=0.95):\n",
        "    \"\"\"\n",
        "    Lo·∫°i b·ªè c√°c boxes tr√πng l·∫∑p d·ª±a tr√™n IoU threshold\n",
        "\n",
        "    Args:\n",
        "        boxes: List of bbox dicts with keys ['x1', 'y1', 'x2', 'y2']\n",
        "        iou_threshold: IoU threshold ƒë·ªÉ x√°c ƒë·ªãnh tr√πng l·∫∑p\n",
        "\n",
        "    Returns:\n",
        "        Filtered list of boxes\n",
        "    \"\"\"\n",
        "    if len(boxes) <= 1:\n",
        "        return boxes\n",
        "\n",
        "    def calculate_iou(box1, box2):\n",
        "        \"\"\"T√≠nh IoU gi·ªØa 2 boxes\"\"\"\n",
        "        x1_min, y1_min, x1_max, y1_max = box1\n",
        "        x2_min, y2_min, x2_max, y2_max = box2\n",
        "\n",
        "        inter_x_min = max(x1_min, x2_min)\n",
        "        inter_y_min = max(y1_min, y2_min)\n",
        "        inter_x_max = min(x1_max, x2_max)\n",
        "        inter_y_max = min(y1_max, y2_max)\n",
        "\n",
        "        if inter_x_max < inter_x_min or inter_y_max < inter_y_min:\n",
        "            return 0.0\n",
        "\n",
        "        inter_area = (inter_x_max - inter_x_min) * (inter_y_max - inter_y_min)\n",
        "        box1_area = (x1_max - x1_min) * (y1_max - y1_min)\n",
        "        box2_area = (x2_max - x2_min) * (y2_max - y2_min)\n",
        "\n",
        "        union_area = box1_area + box2_area - inter_area\n",
        "        return inter_area / union_area if union_area > 0 else 0.0\n",
        "\n",
        "    boxes_xyxy = [(bb['x1'], bb['y1'], bb['x2'], bb['y2']) for bb in boxes]\n",
        "    keep = [True] * len(boxes)\n",
        "\n",
        "    for i in range(len(boxes)):\n",
        "        if not keep[i]:\n",
        "            continue\n",
        "        for j in range(i + 1, len(boxes)):\n",
        "            if not keep[j]:\n",
        "                continue\n",
        "\n",
        "            iou = calculate_iou(boxes_xyxy[i], boxes_xyxy[j])\n",
        "            if iou > iou_threshold:\n",
        "                area_i = (boxes[i]['x2'] - boxes[i]['x1']) * (boxes[i]['y2'] - boxes[i]['y1'])\n",
        "                area_j = (boxes[j]['x2'] - boxes[j]['x1']) * (boxes[j]['y2'] - boxes[j]['y1'])\n",
        "\n",
        "                if area_i >= area_j:\n",
        "                    keep[j] = False\n",
        "                else:\n",
        "                    keep[i] = False\n",
        "                    break\n",
        "\n",
        "    return [boxes[i] for i in range(len(boxes)) if keep[i]]\n",
        "\n",
        "print(\"‚úÖ Duplicate removal function loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "779911f0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "779911f0",
        "outputId": "648a7aac-1729-4c39-a237-c5b7e239b950"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Frame extraction function loaded\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# 4. ENHANCED FRAME EXTRACTION (V·ªõi Masking)\n",
        "# ============================================\n",
        "\n",
        "def extract_frames_with_masking(\n",
        "    video_id,\n",
        "    ann_dict,\n",
        "    mode=\"train\",\n",
        "    augmentation_id=0,\n",
        "    mask_strategy='random',\n",
        "    mask_ratio=0.3\n",
        "):\n",
        "    \"\"\"\n",
        "    Extract frames + labels v·ªõi masked frame augmentation\n",
        "\n",
        "    Args:\n",
        "        video_id: ID c·ªßa video\n",
        "        ann_dict: Dictionary ch·ª©a annotations\n",
        "        mode: \"train\" ho·∫∑c \"val\"\n",
        "        augmentation_id: ID duy nh·∫•t cho augmentation\n",
        "        mask_strategy: Chi·∫øn l∆∞·ª£c mask ('random', 'span', 'keyframe', 'block')\n",
        "        mask_ratio: T·ª∑ l·ªá frames b·ªã mask\n",
        "\n",
        "    Returns:\n",
        "        Dict v·ªõi status v√† th·ªëng k√™\n",
        "    \"\"\"\n",
        "    video_dir = os.path.join(config.SAMPLES_DIR, video_id)\n",
        "    video_path = os.path.join(video_dir, \"drone_video.mp4\")\n",
        "\n",
        "    if not os.path.exists(video_path):\n",
        "        return {'status': 'missing_video', 'video_id': video_id}\n",
        "\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        cap.release()\n",
        "        return {'status': 'cannot_open', 'video_id': video_id}\n",
        "\n",
        "    h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "\n",
        "    # Build bbox_dict v·ªõi deduplication\n",
        "    bbox_dict = {}\n",
        "    dup_removed = 0\n",
        "\n",
        "    for interval in ann_dict.get(video_id, {}).get(\"annotations\", []):\n",
        "        for bb in interval.get(\"bboxes\", []):\n",
        "            frame_idx = bb[\"frame\"]\n",
        "            bbox_dict.setdefault(frame_idx, []).append(bb)\n",
        "\n",
        "    # Remove duplicates\n",
        "    for frame_idx in bbox_dict:\n",
        "        original_count = len(bbox_dict[frame_idx])\n",
        "        bbox_dict[frame_idx] = remove_duplicate_boxes(bbox_dict[frame_idx], iou_threshold=0.95)\n",
        "        dup_removed += original_count - len(bbox_dict[frame_idx])\n",
        "\n",
        "    # Apply masking strategy (if enabled and ratio > 0)\n",
        "    masked_frames = []\n",
        "    ground_truth = {}\n",
        "\n",
        "    if config.ENABLE_MASKING and mask_ratio > 0 and len(bbox_dict) > 4:\n",
        "        masker = FrameMaskingStrategy()\n",
        "\n",
        "        # L·∫•y danh s√°ch c√°c frame_indices c√≥ label\n",
        "        frame_indices = sorted(bbox_dict.keys())\n",
        "        num_frames_with_labels = len(frame_indices)\n",
        "\n",
        "        if mask_strategy == 'random':\n",
        "            masked_idx_list = masker.random_mask(num_frames_with_labels, mask_ratio)\n",
        "        elif mask_strategy == 'span':\n",
        "            masked_idx_list = masker.span_mask(num_frames_with_labels, mask_ratio)\n",
        "        elif mask_strategy == 'keyframe':\n",
        "            # keyframe_mask t·ª± tr·∫£ v·ªÅ frame_idx, kh√¥ng ph·∫£i index\n",
        "            masked_frames = masker.keyframe_mask(bbox_dict, mask_ratio)\n",
        "            masked_idx_list = []  # ƒê√°nh d·∫•u l√† ƒë√£ x·ª≠ l√Ω\n",
        "        elif mask_strategy == 'block':\n",
        "            masked_idx_list = masker.block_mask(num_frames_with_labels, mask_ratio)\n",
        "        else:\n",
        "            masked_idx_list = []\n",
        "\n",
        "        # Chuy·ªÉn ƒë·ªïi t·ª´ list index (0, 1, 2...) sang frame index (120, 125, 130...)\n",
        "        if masked_idx_list:\n",
        "            masked_frames = [frame_indices[i] for i in masked_idx_list\n",
        "                             if i < len(frame_indices)]\n",
        "\n",
        "        # Generate ground truth for masked frames\n",
        "        if masked_frames:\n",
        "            ground_truth, _ = interpolate_boxes(bbox_dict, masked_frames, method='cubic')\n",
        "\n",
        "    # Output directories\n",
        "    if mode == \"train\":\n",
        "        img_out = os.path.join(config.WORK_DIR, 'train', 'images')\n",
        "        lbl_out = os.path.join(config.WORK_DIR, 'train', 'labels')\n",
        "    else:\n",
        "        img_out = os.path.join(config.WORK_DIR, 'val', 'images')\n",
        "        lbl_out = os.path.join(config.WORK_DIR, 'val', 'labels')\n",
        "\n",
        "    saved = 0\n",
        "    masked_count = 0\n",
        "    idx = 0\n",
        "\n",
        "    try:\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            # Ch·ªâ l∆∞u c√°c frame c√≥ trong annotation\n",
        "            if idx % config.FRAME_STEP == 0 and idx in bbox_dict:\n",
        "                # Filename v·ªõi augmentation ID DUY NH·∫§T\n",
        "                img_name = f\"{video_id}_aug{augmentation_id:04d}_frame_{idx:06d}.{config.IMG_EXT}\"\n",
        "                img_path = os.path.join(img_out, img_name)\n",
        "\n",
        "                txt_name = f\"{video_id}_aug{augmentation_id:04d}_frame_{idx:06d}.txt\"\n",
        "                txt_path = os.path.join(lbl_out, txt_name)\n",
        "\n",
        "                # Use ground truth for masked frames, original for visible\n",
        "                if idx in masked_frames and idx in ground_truth:\n",
        "                    boxes_to_save = ground_truth[idx]\n",
        "                    masked_count += 1\n",
        "                elif idx in bbox_dict:\n",
        "                    boxes_to_save = bbox_dict[idx]\n",
        "                else:\n",
        "                    boxes_to_save = []  # Kh√¥ng n√™n x·∫£y ra n·∫øu logic ƒë√∫ng\n",
        "\n",
        "                # Ch·ªâ l∆∞u ·∫£nh n·∫øu frame ƒë√≥ ƒë∆∞·ª£c ch·ªçn\n",
        "                if boxes_to_save:\n",
        "                    if not cv2.imwrite(img_path, frame):\n",
        "                        continue  # Skip if write failed\n",
        "\n",
        "                    lines = []\n",
        "                    for bb in boxes_to_save:\n",
        "                        x1, y1, x2, y2 = bb[\"x1\"], bb[\"y1\"], bb[\"x2\"], bb[\"y2\"]\n",
        "\n",
        "                        # Validate bbox\n",
        "                        if x2 <= x1 or y2 <= y1:\n",
        "                            continue\n",
        "                        if x1 < 0 or y1 < 0 or x2 > w or y2 > h:\n",
        "                            x1, y1 = max(0, x1), max(0, y1)\n",
        "                            x2, y2 = min(w, x2), min(h, y2)\n",
        "\n",
        "                        cx, cy = (x1 + x2) / 2 / w, (y1 + y2) / 2 / h\n",
        "                        bw, bh = (x2 - x1) / w, (y2 - y1) / h\n",
        "\n",
        "                        if not (0 <= cx <= 1 and 0 <= cy <= 1):\n",
        "                            continue\n",
        "                        if not (0 < bw <= 1 and 0 < bh <= 1):\n",
        "                            continue\n",
        "\n",
        "                        lines.append(f\"0 {cx:.6f} {cy:.6f} {bw:.6f} {bh:.6f}\")\n",
        "\n",
        "                    with open(txt_path, \"w\") as f:\n",
        "                        f.write(\"\\n\".join(lines))\n",
        "\n",
        "                    saved += 1\n",
        "\n",
        "            idx += 1\n",
        "    finally:\n",
        "        cap.release()\n",
        "\n",
        "    return {\n",
        "        'status': 'success',\n",
        "        'video_id': video_id,\n",
        "        'aug_id': augmentation_id,\n",
        "        'frames_saved': saved,\n",
        "        'masked_frames': masked_count,\n",
        "        'duplicates_removed': dup_removed\n",
        "    }\n",
        "\n",
        "print(\"‚úÖ Frame extraction function loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3e853a0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3e853a0",
        "outputId": "392a0b14-9401-47f7-e1c2-2be89ccea108"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Curriculum controller loaded\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# 5. CURRICULUM LEARNING CONTROLLER\n",
        "# ============================================\n",
        "\n",
        "class CurriculumController:\n",
        "    \"\"\"ƒêi·ªÅu khi·ªÉn curriculum learning qua c√°c epochs (ch·ªâ ƒë·ªÉ gi√°m s√°t)\"\"\"\n",
        "\n",
        "    def __init__(self, curriculum_config):\n",
        "        self.curriculum = curriculum_config\n",
        "        self.phases = sorted(curriculum_config.items(),\n",
        "                            key=lambda x: x[1]['epochs'][0])\n",
        "\n",
        "    def get_phase(self, epoch):\n",
        "        \"\"\"L·∫•y phase hi·ªán t·∫°i d·ª±a tr√™n epoch\"\"\"\n",
        "        for phase_name, phase_config in self.phases:\n",
        "            start_epoch, end_epoch = phase_config['epochs']\n",
        "            if start_epoch <= epoch < end_epoch:\n",
        "                return phase_name, phase_config\n",
        "\n",
        "        # Return last phase if beyond\n",
        "        if self.phases:\n",
        "            return self.phases[-1][0], self.phases[-1][1]\n",
        "        return None, None\n",
        "\n",
        "    def get_config(self, epoch):\n",
        "        \"\"\"L·∫•y config cho epoch hi·ªán t·∫°i\"\"\"\n",
        "        phase_name, phase_config = self.get_phase(epoch)\n",
        "        if phase_config is None:\n",
        "            return {'phase': 'unknown', 'mask_ratio': 0.0, 'strategy': 'random'}\n",
        "        return {\n",
        "            'phase': phase_name,\n",
        "            'mask_ratio': phase_config['mask_ratio'],\n",
        "            'strategy': phase_config['strategy']\n",
        "        }\n",
        "\n",
        "    def print_schedule(self):\n",
        "        \"\"\"In l·ªãch tr√¨nh curriculum\"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"üìö CURRICULUM LEARNING SCHEDULE (FOR DATA GENERATION)\")\n",
        "        print(\"=\"*60)\n",
        "        for phase_name, phase_config in self.phases:\n",
        "            start, end = phase_config['epochs']\n",
        "            print(f\"\\n{phase_name.upper()}: (Reference for Epochs {start}-{end})\")\n",
        "            print(f\"  Mask Ratio: {phase_config['mask_ratio']*100:.0f}%\")\n",
        "            print(f\"  Strategy: {phase_config['strategy']}\")\n",
        "            print(f\"  Augmentations: {config.NUM_AUGMENTATIONS_PER_PHASE} versions\")\n",
        "        print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "print(\"‚úÖ Curriculum controller loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef9a84fe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ef9a84fe",
        "outputId": "6b055fd4-4008-4d0e-c47e-2a2a84066e50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Model analysis function loaded\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# MODEL ANALYSIS UTILITY\n",
        "# ============================================\n",
        "\n",
        "def analyze_model(model_path):\n",
        "    \"\"\"Ph√¢n t√≠ch chi ti·∫øt model\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üîç MODEL ANALYSIS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    model = YOLOWorld(model_path)\n",
        "    model.set_classes(config.CLASS_NAMES)\n",
        "\n",
        "    optimizer = ModelOptimizer(model)\n",
        "    params = optimizer.count_parameters()\n",
        "\n",
        "    # Detailed layer analysis\n",
        "    print(\"\\nüìä Layer-wise Parameter Count:\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    layer_params = {}\n",
        "    for name, module in model.model.named_modules():\n",
        "        if len(list(module.children())) == 0:  # Leaf modules only\n",
        "            num_params = sum(p.numel() for p in module.parameters())\n",
        "            if num_params > 0:\n",
        "                layer_type = type(module).__name__\n",
        "                if layer_type not in layer_params:\n",
        "                    layer_params[layer_type] = {'count': 0, 'params': 0}\n",
        "                layer_params[layer_type]['count'] += 1\n",
        "                layer_params[layer_type]['params'] += num_params\n",
        "\n",
        "    # Sort by parameter count\n",
        "    sorted_layers = sorted(layer_params.items(), key=lambda x: x[1]['params'], reverse=True)\n",
        "\n",
        "    for layer_type, info in sorted_layers[:10]:  # Top 10\n",
        "        print(f\"   {layer_type:20s}: {info['count']:3d} layers, {info['params']:12,d} params \"\n",
        "              f\"({info['params']/params['total']*100:5.2f}%)\")\n",
        "\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"   {'TOTAL':20s}: {params['total']:,} params\\n\")\n",
        "\n",
        "    # Memory estimation\n",
        "    print(\"üíæ Memory Estimates:\")\n",
        "    print(f\"   FP32 (training): {params['total'] * 4 / 1024**2:.2f} MB\")\n",
        "    print(f\"   FP16 (inference): {params['total'] * 2 / 1024**2:.2f} MB\")\n",
        "    print(f\"   INT8 (optimized): {params['total'] * 1 / 1024**2:.2f} MB\")\n",
        "\n",
        "    # Check limit\n",
        "    print(f\"\\n‚öñÔ∏è  Parameter Limit Check:\")\n",
        "    if params['total'] <= config.PARAM_LIMIT:\n",
        "        print(f\"   ‚úÖ Within limit: {params['total']:,} / {config.PARAM_LIMIT:,}\")\n",
        "        print(f\"   Remaining: {config.PARAM_LIMIT - params['total']:,} params\")\n",
        "    else:\n",
        "        print(f\"   ‚ùå Exceeds limit: {params['total']:,} / {config.PARAM_LIMIT:,}\")\n",
        "        print(f\"   Over by: {params['total'] - config.PARAM_LIMIT:,} params\")\n",
        "        print(f\"\\n   Recommended actions:\")\n",
        "        print(f\"   1. Apply pruning (30-40% reduction)\")\n",
        "        print(f\"   2. Use smaller base model (yolov8n)\")\n",
        "        print(f\"   3. Reduce model depth/width\")\n",
        "\n",
        "    print(\"=\"*60)\n",
        "\n",
        "print(\"‚úÖ Model analysis function loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b0c2c5a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8b0c2c5a",
        "outputId": "1c2cb426-ff08-4284-b647-0c5264537de4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Main pipeline function loaded\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# 6. MAIN PIPELINE\n",
        "# ============================================\n",
        "\n",
        "def main_pipeline():\n",
        "    \"\"\"Main training pipeline v·ªõi curriculum learning (Mixed Dataset)\"\"\"\n",
        "\n",
        "    print(\"üöÄ ENHANCED YOLO WORLD TRAINING PIPELINE (MIXED DATA)\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Load annotations\n",
        "    with open(config.ANNOTATIONS_PATH, \"r\") as f:\n",
        "        annotations = json.load(f)\n",
        "\n",
        "    video_ids = [a[\"video_id\"] for a in annotations]\n",
        "    ann_dict = {a[\"video_id\"]: a for a in annotations}\n",
        "\n",
        "    # Split train/val\n",
        "    random.seed(42)\n",
        "    random.shuffle(video_ids)\n",
        "    split_idx = int(len(video_ids) * config.TRAIN_RATIO)\n",
        "    train_videos = video_ids[:split_idx]\n",
        "    val_videos = video_ids[split_idx:]\n",
        "\n",
        "    print(f\"‚úÖ Loaded {len(video_ids)} videos\")\n",
        "    print(f\"    Train: {len(train_videos)}, Val: {len(val_videos)}\")\n",
        "\n",
        "    # Setup curriculum\n",
        "    curriculum = CurriculumController(config.CURRICULUM)\n",
        "    curriculum.print_schedule()\n",
        "\n",
        "    # Create output directories\n",
        "    os.makedirs(config.WORK_DIR, exist_ok=True)\n",
        "    for subdir in ['train/images', 'train/labels', 'val/images', 'val/labels']:\n",
        "        os.makedirs(os.path.join(config.WORK_DIR, subdir), exist_ok=True)\n",
        "\n",
        "    # T·∫°o m·ªôt danh s√°ch job duy nh·∫•t ch·ª©a T·∫§T C·∫¢ c√°c augmentation\n",
        "    print(f\"\\n‚öôÔ∏è Preparing all data augmentations in one go...\")\n",
        "    futures = []\n",
        "    stats_list = []\n",
        "    global_aug_id_counter = 0  # ID tƒÉng c∆∞·ªùng duy nh·∫•t\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=config.NUM_WORKERS) as ex:\n",
        "\n",
        "        # 1. X·ª≠ l√Ω TRAIN data v·ªõi T·∫§T C·∫¢ c√°c phase curriculum\n",
        "        for phase_name, phase_config in curriculum.phases:\n",
        "            print(f\"  -> Submitting jobs for Phase: {phase_name.upper()} \"\n",
        "                  f\"({config.NUM_AUGMENTATIONS_PER_PHASE} versions each)\")\n",
        "\n",
        "            mask_ratio = phase_config['mask_ratio']\n",
        "            mask_strategy = phase_config['strategy']\n",
        "\n",
        "            for vid in train_videos:\n",
        "                for _ in range(config.NUM_AUGMENTATIONS_PER_PHASE):\n",
        "                    futures.append(\n",
        "                        ex.submit(\n",
        "                            extract_frames_with_masking,\n",
        "                            vid, ann_dict, \"train\",\n",
        "                            global_aug_id_counter,\n",
        "                            mask_strategy, mask_ratio\n",
        "                        )\n",
        "                    )\n",
        "                    global_aug_id_counter += 1\n",
        "\n",
        "        print(f\"\\n  -> Submitting jobs for VALIDATION data (1 version, no masking)\")\n",
        "        # 2. X·ª≠ l√Ω VAL data (ch·ªâ 1 l·∫ßn, kh√¥ng mask, aug_id = 0)\n",
        "        for vid in val_videos:\n",
        "            futures.append(\n",
        "                ex.submit(\n",
        "                    extract_frames_with_masking,\n",
        "                    vid, ann_dict, \"val\",\n",
        "                    0,  # aug_id 0 cho val\n",
        "                    'random', 0.0  # mask_ratio = 0.0\n",
        "                )\n",
        "            )\n",
        "\n",
        "        print(f\"\\n‚è≥ Waiting for {len(futures)} total jobs to complete...\")\n",
        "        # 3. Thu th·∫≠p k·∫øt qu·∫£\n",
        "        for i, fut in enumerate(as_completed(futures), 1):\n",
        "            result = fut.result()\n",
        "            stats_list.append(result)\n",
        "\n",
        "            if i % 100 == 0 or i == len(futures):\n",
        "                if result['status'] == 'success':\n",
        "                    msg = (f\"[{i}/{len(futures)}] ‚úÖ {result['video_id']}_aug{result['aug_id']:04d}: \"\n",
        "                           f\"{result['frames_saved']} frames\")\n",
        "                    if result['masked_frames'] > 0:\n",
        "                        msg += f\" ({result['masked_frames']} masked)\"\n",
        "                    print(msg)\n",
        "                else:\n",
        "                    print(f\"[{i}/{len(futures)}] ‚ö†Ô∏è {result['video_id']}: {result['status']}\")\n",
        "\n",
        "    # In th·ªëng k√™ t·ªïng\n",
        "    success_stats = [s for s in stats_list if s['status'] == 'success']\n",
        "    total_frames = sum(s['frames_saved'] for s in success_stats)\n",
        "    total_masked = sum(s['masked_frames'] for s in success_stats)\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"üìà TOTAL DATASET STATISTICS (ALL PHASES MIXED):\")\n",
        "    print(f\"   Total frames saved: {total_frames}\")\n",
        "    print(f\"   Total masked frames: {total_masked} ({total_masked/max(1,total_frames)*100:.1f}%)\")\n",
        "    print(f\"   Total train augmentations: {global_aug_id_counter}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Create data.yaml\n",
        "    data_yaml = {\n",
        "        \"train\": os.path.abspath(os.path.join(config.WORK_DIR, 'train', 'images')),\n",
        "        \"val\": os.path.abspath(os.path.join(config.WORK_DIR, 'val', 'images')),\n",
        "        \"nc\": 1,\n",
        "        \"names\": config.CLASS_NAMES\n",
        "    }\n",
        "    data_path = os.path.join(config.WORK_DIR, \"data.yaml\")\n",
        "    with open(data_path, \"w\") as f:\n",
        "        yaml.dump(data_yaml, f)\n",
        "\n",
        "    print(f\"\\nüìÑ data.yaml created at: {data_path}\")\n",
        "    print(open(data_path).read())\n",
        "\n",
        "    # Training\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üöÄ STARTING TRAINING ON MIXED DATASET\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    model = YOLOWorld(config.MODEL_WEIGHTS)\n",
        "    model.set_classes(config.CLASS_NAMES)\n",
        "\n",
        "    # Custom training callback (ch·ªâ ƒë·ªÉ gi√°m s√°t)\n",
        "    monitor = CurriculumController(config.CURRICULUM)\n",
        "\n",
        "    def on_epoch_end(trainer):\n",
        "        \"\"\"Callback ƒë·ªÉ IN RA phase t∆∞∆°ng ·ª©ng v·ªõi epoch\"\"\"\n",
        "        epoch = trainer.epoch + 1  # trainer.epoch l√† 0-indexed\n",
        "        phase_config = monitor.get_config(epoch)\n",
        "\n",
        "        if epoch == 1 or (epoch) % 5 == 0 or epoch == trainer.epochs:\n",
        "            print(f\"\\n-- üìö Epoch {epoch}/{trainer.epochs} -- \"\n",
        "                  f\"Curriculum Phase (Reference): {phase_config['phase']} --\")\n",
        "\n",
        "    # Th√™m callback TR∆Ø·ªöC KHI train\n",
        "    model.add_callback(\"on_epoch_end\", on_epoch_end)\n",
        "\n",
        "    # Train\n",
        "    results = model.train(\n",
        "        data=data_path,\n",
        "        epochs=50,\n",
        "        imgsz=896,\n",
        "        batch=32,\n",
        "        lr0=5e-4,\n",
        "        optimizer=\"AdamW\",\n",
        "        box=10.0,\n",
        "        cls=0.5,\n",
        "        dfl=1.5,\n",
        "\n",
        "        # Data augmentation (temporal-friendly)\n",
        "        mosaic=0.3,  # Gi·∫£m mosaic\n",
        "        mixup=0.0,\n",
        "        copy_paste=0.0,\n",
        "        degrees=10.0,\n",
        "        translate=0.1,\n",
        "        scale=0.5,\n",
        "        flipud=0.0,\n",
        "        fliplr=0.5,\n",
        "\n",
        "        # Regularization\n",
        "        dropout=0.0,\n",
        "        weight_decay=0.0005,\n",
        "\n",
        "        # Training settings\n",
        "        patience=10,\n",
        "        save_period=5,\n",
        "        workers=8,\n",
        "        close_mosaic=10,\n",
        "\n",
        "        # Output\n",
        "        project=os.path.join(config.WORK_DIR, \"runs\"),\n",
        "        name=\"curriculum_MIXED_training\",\n",
        "        exist_ok=True,\n",
        "        pretrained=True,\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"‚úÖ TRAINING COMPLETE!\")\n",
        "    print(\"=\"*60)\n",
        "    best_model_path = os.path.join(model.trainer.save_dir, 'weights', 'best.pt')\n",
        "    print(f\"Best model: {best_model_path}\")\n",
        "\n",
        "    return best_model_path\n",
        "\n",
        "print(\"‚úÖ Main pipeline function loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba793312",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ba793312",
        "outputId": "df876829-ec40-4b25-c707-146c0eddea4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Inference function loaded\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# 7. INFERENCE\n",
        "# ============================================\n",
        "\n",
        "def run_inference(model_path, test_root, output_json, conf=0.35, iou=0.55):\n",
        "    \"\"\"Run inference tr√™n test set\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üéØ RUNNING INFERENCE\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model = YOLOWorld(model_path)\n",
        "    model.set_classes(config.CLASS_NAMES)\n",
        "\n",
        "    video_dirs = sorted([d for d in os.listdir(test_root)\n",
        "                         if os.path.isdir(os.path.join(test_root, d))])\n",
        "    submission = []\n",
        "\n",
        "    for i, vid in enumerate(video_dirs, 1):\n",
        "        video_path = os.path.join(test_root, vid, \"drone_video.mp4\")\n",
        "        if not os.path.exists(video_path):\n",
        "            continue\n",
        "\n",
        "        bboxes_per_frame = []\n",
        "\n",
        "        results = model.predict(\n",
        "            source=video_path,\n",
        "            conf=conf,\n",
        "            iou=iou,\n",
        "            imgsz=896,\n",
        "            stream=True,\n",
        "            verbose=False,\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "        frame_idx = 0\n",
        "        for r in results:\n",
        "            if r.boxes is not None and len(r.boxes) > 0:\n",
        "                boxes = r.boxes.xyxy.cpu().numpy()\n",
        "                for (x1, y1, x2, y2) in boxes:\n",
        "                    bboxes_per_frame.append({\n",
        "                        \"frame\": frame_idx,\n",
        "                        \"x1\": int(x1),\n",
        "                        \"y1\": int(y1),\n",
        "                        \"x2\": int(x2),\n",
        "                        \"y2\": int(y2)\n",
        "                    })\n",
        "            frame_idx += 1\n",
        "\n",
        "        submission.append({\n",
        "            \"video_id\": vid,\n",
        "            \"detections\": [{\"bboxes\": bboxes_per_frame}] if bboxes_per_frame else []\n",
        "        })\n",
        "\n",
        "        print(f\"[{i}/{len(video_dirs)}] {vid}: {len(bboxes_per_frame)} detections\")\n",
        "\n",
        "    with open(output_json, \"w\") as f:\n",
        "        json.dump(submission, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"\\n‚úÖ Saved submission to: {output_json}\")\n",
        "\n",
        "print(\"‚úÖ Inference function loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "060631c0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "060631c0",
        "outputId": "daac56e6-41e3-4e66-b9f2-190303b2e6b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
            "‚ïë    YOLO WORLD + MASKED TRAINING + CURRICULUM (MIXED)    ‚ïë\n",
            "‚ïë    Enhanced Pipeline for Video Object Detection         ‚ïë\n",
            "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
            "\n",
            "üñ•Ô∏è  Device: CUDA\n",
            "    GPU: NVIDIA A100-SXM4-40GB\n",
            "üöÄ ENHANCED YOLO WORLD TRAINING PIPELINE (MIXED DATA)\n",
            "============================================================\n",
            "‚úÖ Loaded 14 videos\n",
            "    Train: 11, Val: 3\n",
            "\n",
            "============================================================\n",
            "üìö CURRICULUM LEARNING SCHEDULE (FOR DATA GENERATION)\n",
            "============================================================\n",
            "\n",
            "PHASE1: (Reference for Epochs 0-15)\n",
            "  Mask Ratio: 10%\n",
            "  Strategy: random\n",
            "  Augmentations: 5 versions\n",
            "\n",
            "PHASE2: (Reference for Epochs 15-35)\n",
            "  Mask Ratio: 30%\n",
            "  Strategy: span\n",
            "  Augmentations: 5 versions\n",
            "\n",
            "PHASE3: (Reference for Epochs 35-50)\n",
            "  Mask Ratio: 50%\n",
            "  Strategy: keyframe\n",
            "  Augmentations: 5 versions\n",
            "============================================================\n",
            "\n",
            "\n",
            "‚öôÔ∏è Preparing all data augmentations in one go...\n",
            "  -> Submitting jobs for Phase: PHASE1 (5 versions each)\n",
            "  -> Submitting jobs for Phase: PHASE2 (5 versions each)\n",
            "  -> Submitting jobs for Phase: PHASE3 (5 versions each)\n",
            "\n",
            "  -> Submitting jobs for VALIDATION data (1 version, no masking)\n",
            "\n",
            "‚è≥ Waiting for 168 total jobs to complete...\n",
            "[100/168] ‚úÖ MobilePhone_1_aug0099: 889 frames (264 masked)\n",
            "[168/168] ‚úÖ Backpack_0_aug0000: 3184 frames\n",
            "\n",
            "============================================================\n",
            "üìà TOTAL DATASET STATISTICS (ALL PHASES MIXED):\n",
            "   Total frames saved: 207860\n",
            "   Total masked frames: 57536 (27.7%)\n",
            "   Total train augmentations: 165\n",
            "============================================================\n",
            "\n",
            "üìÑ data.yaml created at: enhanced_mixed_dataset/data.yaml\n",
            "names:\n",
            "- target\n",
            "nc: 1\n",
            "train: /content/enhanced_mixed_dataset/train/images\n",
            "val: /content/enhanced_mixed_dataset/val/images\n",
            "\n",
            "\n",
            "============================================================\n",
            "üöÄ STARTING TRAINING ON MIXED DATASET\n",
            "============================================================\n",
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8s-worldv2.pt to 'yolov8s-worldv2.pt': 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 24.7MB 208.8MB/s 0.1s\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['git+https://github.com/ultralytics/CLIP.git'] not found, attempting AutoUpdate...\n",
            "Using Python 3.12.12 environment at: /usr\n",
            "Resolved 33 packages in 489ms\n",
            "Prepared 2 packages in 2.11s\n",
            "Installed 2 packages in 1ms\n",
            " + clip==1.0 (from git+https://github.com/ultralytics/CLIP.git@19870eafbf315e6bfbc73fc97935abbd83924416)\n",
            " + ftfy==6.3.1\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ‚úÖ 3.1s\n",
            "WARNING ‚ö†Ô∏è \u001b[31m\u001b[1mrequirements:\u001b[0m \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 338M/338M [00:02<00:00, 123MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.228 üöÄ Python-3.12.12 torch-2.8.0+cu126 CUDA:0 (NVIDIA A100-SXM4-40GB, 40507MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=32, bgr=0.0, box=10.0, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=enhanced_mixed_dataset/data.yaml, degrees=10.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=50, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=896, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.0005, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8s-worldv2.pt, momentum=0.937, mosaic=0.3, multi_scale=False, name=curriculum_MIXED_training, nbs=64, nms=False, opset=None, optimize=False, optimizer=AdamW, overlap_mask=True, patience=15, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=enhanced_mixed_dataset/runs, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/content/enhanced_mixed_dataset/runs/curriculum_MIXED_training, save_frames=False, save_json=False, save_period=5, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "\u001b[KDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf': 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 755.1KB 16.3MB/s 0.0s\n",
            "Overriding model.yaml nc=80 with nc=1\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
            "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
            "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
            "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
            "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
            "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    837508  ultralytics.nn.modules.block.C2fAttn         [768, 256, 1, 128, 4]         \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1    226242  ultralytics.nn.modules.block.C2fAttn         [384, 128, 1, 64, 2]          \n",
            " 16                  15  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    739204  ultralytics.nn.modules.block.C2fAttn         [384, 256, 1, 128, 4]         \n",
            " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1   2821896  ultralytics.nn.modules.block.C2fAttn         [768, 512, 1, 256, 8]         \n",
            " 22        [15, 18, 21]  1   2317270  ultralytics.nn.modules.head.WorldDetect      [1, 512, True, [128, 256, 512]]\n",
            "YOLOv8s-worldv2 summary: 148 layers, 12,759,880 parameters, 12,759,864 gradients, 32.3 GFLOPs\n",
            "\n",
            "Transferred 409/412 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt': 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5.4MB 72.9MB/s 0.1s\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 2614.4¬±1735.7 MB/s, size: 253.4 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/enhanced_mixed_dataset/train/labels... 201165 images, 7352 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 201165/201165 982.5it/s 3:25\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/enhanced_mixed_dataset/train/labels.cache\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "Caching text embeddings to '/content/enhanced_mixed_dataset/train/text_embeddings_clip_ViT-B_32.pt'\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 2148.1¬±1686.2 MB/s, size: 277.7 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/enhanced_mixed_dataset/val/labels... 6695 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6695/6695 1.2Kit/s 5.5s\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/enhanced_mixed_dataset/val/labels.cache\n",
            "Plotting labels to /content/enhanced_mixed_dataset/runs/curriculum_MIXED_training/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.0005, momentum=0.937) with parameter groups 67 weight(decay=0.0), 72 weight(decay=0.0005), 81 bias(decay=0.0)\n",
            "Image sizes 896 train, 896 val\n",
            "Using 8 dataloader workers\n",
            "Logging results to \u001b[1m/content/enhanced_mixed_dataset/runs/curriculum_MIXED_training\u001b[0m\n",
            "Starting training for 50 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       1/50        15G      1.509      0.715      1.101         20        896: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6287/6287 3.6it/s 29:04\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 105/105 4.1it/s 25.8s\n",
            "                   all       6695       6695      0.785       0.65      0.731      0.368\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       2/50      15.8G      1.334      0.626      1.043         15        896: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6287/6287 3.7it/s 28:37\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 105/105 4.7it/s 22.4s\n",
            "                   all       6695       6695      0.876       0.58      0.676      0.382\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       3/50      15.8G      1.252     0.5948      1.016          9        896: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6287/6287 3.7it/s 28:05\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 105/105 4.6it/s 22.7s\n",
            "                   all       6695       6695      0.823      0.508      0.611      0.329\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       4/50      15.9G      1.178     0.5633     0.9932         16        896: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6287/6287 3.7it/s 28:01\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 105/105 4.8it/s 22.1s\n",
            "                   all       6695       6695      0.871      0.458      0.616      0.325\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       5/50      15.9G      1.114     0.5376     0.9734         14        896: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6287/6287 3.7it/s 27:58\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 105/105 5.0it/s 20.8s\n",
            "                   all       6695       6695      0.557      0.574       0.62      0.332\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       6/50        16G      1.066      0.519     0.9601         17        896: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6287/6287 3.8it/s 27:56\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 105/105 5.3it/s 20.0s\n",
            "                   all       6695       6695      0.664      0.499      0.603      0.323\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       7/50        16G      1.046     0.5114     0.9569         36        896: 12% ‚îÅ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 779/6287 4.2it/s 3:28<21:41\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-16 (_pin_memory_loop):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1075, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1012, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py\", line 61, in _pin_memory_loop\n",
            "    do_one_step()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py\", line 37, in do_one_step\n",
            "    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/multiprocessing/queues.py\", line 122, in get\n",
            "    return _ForkingPickler.loads(res)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/multiprocessing/reductions.py\", line 541, in rebuild_storage_fd\n",
            "    fd = df.detach()\n",
            "         ^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/multiprocessing/resource_sharer.py\", line 57, in detach\n",
            "    with _resource_sharer.get_connection(self._id) as conn:\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/multiprocessing/resource_sharer.py\", line 86, in get_connection\n",
            "    c = Client(address, authkey=process.current_process().authkey)\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 519, in Client\n",
            "    c = SocketClient(address)\n",
            "        ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 647, in SocketClient\n",
            "    s.connect(address)\n",
            "FileNotFoundError: [Errno 2] No such file or directory\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-872094935.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Run main pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mbest_model_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# Optional: Run inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3223726445.py\u001b[0m in \u001b[0;36mmain_pipeline\u001b[0;34m()\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m     results = model.train(\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/engine/model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[1;32m    776\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    779\u001b[0m         \u001b[0;31m# Update model and cfg after training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mRANK\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/engine/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setup_scheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/engine/trainer.py\u001b[0m in \u001b[0;36m_do_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m                 \u001b[0;31m# Backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mni\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlast_opt_step\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m             )\n\u001b[0;32m--> 647\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    648\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    830\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# 8. MAIN EXECUTION\n",
        "# ============================================\n",
        "\n",
        "print(\"\"\"\n",
        "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
        "‚ïë    YOLO WORLD + MASKED TRAINING + CURRICULUM (MIXED)    ‚ïë\n",
        "‚ïë    Enhanced Pipeline for Video Object Detection         ‚ïë\n",
        "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
        "\"\"\")\n",
        "\n",
        "# Check GPU\n",
        "print(f\"üñ•Ô∏è  Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"    GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "# C·∫•u h√¨nh ƒë∆∞·ªùng d·∫´n\n",
        "config.DATASET_ROOT = \"train\"  # Thay ƒë·ªïi n·∫øu c·∫ßn\n",
        "TEST_ROOT = \"public_test/samples\"  # Thay ƒë·ªïi n·∫øu c·∫ßn\n",
        "OUTPUT_JSON = \"submission_mixed.json\"\n",
        "\n",
        "# Run main pipeline\n",
        "try:\n",
        "    best_model_path = main_pipeline()\n",
        "\n",
        "    # Optional: Run inference\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "    if os.path.exists(TEST_ROOT) and best_model_path:\n",
        "        print(\"Auto-running inference on test set...\")\n",
        "        run_inference(\n",
        "            model_path=best_model_path,\n",
        "            test_root=TEST_ROOT,\n",
        "            output_json=OUTPUT_JSON\n",
        "        )\n",
        "    else:\n",
        "        print(\"Skipping inference: Test set path not found or training failed.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå An error occurred in the pipeline: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}